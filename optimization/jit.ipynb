{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import jacobian\n",
    "from numba import jit, float64, int64\n",
    "import time\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posi_defnite_check(A):\n",
    "    \"\"\"\n",
    "    check whether the matrix is positive definite\n",
    "    \"\"\"\n",
    "    return np.all(np.linalg.eigvals(A) > 0)\n",
    "\n",
    "def batch_fun(data, size):\n",
    "    \"\"\"\n",
    "    function for data batch\n",
    "    \"\"\"\n",
    "    n = data.shape[0]\n",
    "    i = np.arange(n)\n",
    "    p = data.shape[1]\n",
    "    \n",
    "    ## make sure the rows can be divided by the input size\n",
    "    if n % size !=0:\n",
    "        print(\"The number of observations of data cannot be divided by the input batch size.\")\n",
    "        print('%d data dropped.' % (n%size))\n",
    "        \n",
    "    sample_size = (n // size) * size\n",
    "        \n",
    "\n",
    "    np.random.shuffle(i)\n",
    "\n",
    "    batch_nums = n//size\n",
    "    data = data[i][:sample_size].reshape(size,p,batch_nums)\n",
    "    return(data, batch_nums)\n",
    "\n",
    "def sghmc(gradU, eps, C, Mmatrix, theta_initial, Cov_hat, epoch_nums, epoch_nums_drop, data, size, seed=663):\n",
    "    \"\"\"\n",
    "    function to apply sghmc \n",
    "    \"\"\"\n",
    "    \n",
    "    ## gradU: function(theta, X, y), U gradient\n",
    "\n",
    "    ## eps: learning rate\n",
    "\n",
    "    ## C: friction matrix P X P\n",
    "\n",
    "    ## Mmatrix: Mass matrix P X P\n",
    "\n",
    "    ## theta_initial: initial value of theta\n",
    "\n",
    "    ## Cov_hat: estimated covariance matrix of stochastic gradient noise\n",
    "\n",
    "    ## epoch_nums: number of epochs to perform\n",
    "\n",
    "    ## epoch_nums_drop: number of epochs to drop\n",
    "\n",
    "    ## size: minbatch size per iteration\n",
    "\n",
    "    ## seed: seed\n",
    "    np.random.seed(seed)\n",
    "    n = data.shape[0]\n",
    "    p = theta_initial.shape[0]\n",
    "\n",
    "    theta_samp = np.zeros((p, epoch_nums))\n",
    "    theta_samp[:, 0] = theta_initial\n",
    "\n",
    "    B_hat = 0.5 * eps * Cov_hat\n",
    "\n",
    "    if not posi_defnite_check(2 * (C - B_hat) * eps):\n",
    "        print(\"The noise term has to be positive define.\")\n",
    "        return\n",
    "\n",
    "    noise_sqrt = np.linalg.cholesky(2 * (C - B_hat) * eps)\n",
    "\n",
    "    r = (np.linalg.cholesky(np.linalg.inv(Mmatrix))) @ np.random.normal(size=p).reshape(p, -1)\n",
    "    data_batched = batch_fun(data, size)[0]\n",
    "    batch_nums = batch_fun(data, size)[1]\n",
    "\n",
    "    for i in range(epoch_nums - 1):\n",
    "        r = (np.linalg.cholesky(np.linalg.inv(Mmatrix))) @ np.random.normal(size=p).reshape(p, -1)\n",
    "        theta = theta_samp[:, i]\n",
    "\n",
    "        for batch in range(batch_nums):\n",
    "            theta += (eps * Mmatrix @ r).ravel()\n",
    "            gradU_batch = gradU(theta, data_batched[:, :, batch], n, size).reshape(p, -1)\n",
    "            r = r - eps * gradU_batch - eps * C @ Mmatrix @ r + np.random.multivariate_normal(np.zeros(p), noise_sqrt).reshape(p, -1)\n",
    "           \n",
    "        theta_samp[:, i + 1] = theta\n",
    "\n",
    "    return theta_samp[:, epoch_nums_drop:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def batch_numba(data, size,seed=663):\n",
    "    \"\"\"\n",
    "    function for data batch\n",
    "    \"\"\"\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    i = np.arange(n)\n",
    "    if n % size !=0:\n",
    "        print(\"The number of observations of data cannot be divided by the input batch size.\")\n",
    "        print('%d data dropped.' % (n%size))\n",
    "        \n",
    "    sample_size = (n // size)*size\n",
    "        \n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(i)\n",
    "    batch_nums = n//size\n",
    "    data = data[i][:sample_size].reshape(size, p, batch_nums)\n",
    "\n",
    "    return(data, batch_nums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def posi_defnite_check(A):\n",
    "    \"\"\"\n",
    "    check whether the matrix is positive definite - jit\n",
    "    \"\"\"\n",
    "    return np.all(np.linalg.eigvals(A) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(cache = True)\n",
    "\n",
    "def sghmc_numba(gradU, eps, C, Mmatrix, theta_initial, Cov_hat, epoch_nums, epoch_nums_drop, data, size,seed=663):\n",
    "    \"\"\"\n",
    "    function to apply sghmc with jit\n",
    "    \"\"\"\n",
    "    \n",
    "        ## gradU: function(theta, X, y), U gradient\n",
    "        \n",
    "        ## eps: learning rate\n",
    "        \n",
    "        ## C: friction matrix P X P\n",
    "        \n",
    "        ## Mmatrix: Mass matrix P X P\n",
    "        \n",
    "        ## theta_initial: initial value of theta\n",
    "        \n",
    "        ## Cov_hat: estimated covariance matrix of stochastic gradient noise\n",
    "        \n",
    "        ## epoch_nums: number of epochs to perform\n",
    "        \n",
    "        ## epoch_nums_drop: number of epochs to drop\n",
    "        \n",
    "        ## size: minbatch size per iteration\n",
    "    \n",
    "    random.seed(seed)\n",
    "    n = data.shape[0]\n",
    "    p = theta_initial.shape[0]\n",
    "\n",
    "    theta_samp = np.zeros((p, epoch_nums))\n",
    "    theta_samp[:,0] = theta_initial\n",
    "    \n",
    "    B_hat = 0.5*eps*Cov_hat\n",
    "    \n",
    "    if not posi_defnite_check(2*(C-B_hat)*eps):\n",
    "        print(\"The noise term has to be positive define.\")\n",
    "        return\n",
    "    \n",
    "    noise_sqrt = np.linalg.cholesky(2*(C-B_hat)*eps)\n",
    "    \n",
    "    r = (np.linalg.cholesky(np.linalg.inv(Mmatrix)))@np.random.normal(size = p).reshape(p, -1)\n",
    "    data_batched = batch_numba(data, size)[0]\n",
    "    batch_nums = batch_numba(data, size)[1]\n",
    "    \n",
    "    for i in range(epoch_nums-1):\n",
    "        r = (np.linalg.cholesky(np.linalg.inv(Mmatrix)))@np.random.normal(size = p).reshape(p, -1)\n",
    "        theta = theta_samp[:,i]\n",
    "        \n",
    "        for batch in range(batch_nums):\n",
    "            theta += (eps * Mmatrix @ r).ravel()\n",
    "            gradU_batch = gradU(theta, data_batched[:,:, batch], n, size).reshape(p, -1)\n",
    "            r = r-eps*gradU_batch - eps*C@Mmatrix@r + noise_sqrt@np.random.normal(size = p).reshape(p, -1)\n",
    "        theta_samp[:,i+1] = theta\n",
    "    \n",
    "    return theta_samp[:, epoch_nums_drop:]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.array([-3,3]).reshape(2,1)\n",
    "\n",
    "def prior1(theta):\n",
    "    return ((-1/(2*10))*theta.T@theta)\n",
    "\n",
    "def p_data(theta, x):\n",
    "    return (np.log(0.5 * np.exp(-0.5*(theta[0]-x)**2) + 0.5* np.exp(-0.5*(theta[1]-x)**2)))\n",
    "\n",
    "def U(theta, x, n, size):\n",
    "    return (-prior1(theta) - (n/size)*sum(p_data(theta, x)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def prior1_numba(theta):\n",
    "    return ((-1/(2*10))*theta.T@theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def p_data_numba(theta, x):\n",
    "    return (np.log(0.5 * np.exp(-0.5*(theta[0]-x)**2) + 0.5* np.exp(-0.5*(theta[1]-x)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def U_numba(theta, x, n, size):\n",
    "    return (-prior1_numba(theta) - (n/size)*sum(p_data_numba(theta, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradU = jacobian(U, argnum = 0)\n",
    "\n",
    "gradU_numba = jacobian(U_numba, argnum = 0)\n",
    "\n",
    "np.random.seed(123)\n",
    "n = 100\n",
    "x = np.r_[\n",
    "    np.random.normal(mu[0], 1, n),\n",
    "    np.random.normal(mu[1], 1, n)].reshape(-1,1)\n",
    "\n",
    "theta_0 = np.array([-3, 3])\n",
    "eps = 0.01\n",
    "V_hat = np.eye(2)\n",
    "C = np.eye(2)\n",
    "epochs = 500\n",
    "burns = 200\n",
    "size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.61 s ± 140 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "7.14 s ± 88.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc(gradU, eps, C, np.eye(2), theta_0, V_hat, epochs, burns, x, size)\n",
    "%timeit sghmc_numba(gradU, eps, C, np.eye(2), theta_0, V_hat, epochs, burns, x, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
